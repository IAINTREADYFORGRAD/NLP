
#DISC classifier
stopwords=[]
for stopword in open(file="DISC_stopwords.txt", mode="r", encoding="utf8"):
    stopwords.append(stopword.strip()) #.strip('x') to cut off all the char x at the beginning and the end of the str
                                       #.strip() is to remove blank spaces at the beginning/end of the str
                                       #remove .strip() \n would show up


#remove stopwords 
personality=['D','I','S','C']
personalityfile=['d', 'i', 's','c']
personalityok=['Dok', 'Iok', 'Sok','Cok']
totality=49
for i in range(0, 4):    
    for j in range (1, totality+1):
        file_txt1="%c\%c%d.txt" %(personality[i], personalityfile[i], j)
        with open(file=file_txt1, mode="r", encoding="utf8") as file1:
            one_txt=file1.read().replace("\n", " ") 
            cutword = psg.cut(one_txt) 

            file_txt2="%s\%s%d.txt" %(personalityok[i], personalityok[i], j)
            with open(file=file_txt2, mode="w", encoding="utf8") as file2:
                words=[]
                for word in cutword:
                    if word.word not in stopwords:
                        words.append(word.word)
                file2.write(" ".join(words).strip()) #.join: to add new elements in a str
                    
#the keywords of the target might be included in the stopwords 
#stopwords emit the words based on the part of speech                      
#pos, x, means unknown
#pas, eng, indicates english. namely the system is supposed to deal with chinese
#it's the problem of encoding. utf8 is meant for encoding Chinese



#D
c="Dok"
pcr_D = PlaintextCorpusReader(root=c, fileids=".*\.txt") 
D_doc = [(pcr_D.words(fileid),c) for fileid in pcr_D.fileids()]

#I
c="Iok"
pcr_I = PlaintextCorpusReader(root=c, fileids=".*\.txt") 
I_doc = [(pcr_I.words(fileid),c) for fileid in pcr_I.fileids()]

#S
c="Sok"
pcr_S = PlaintextCorpusReader(root=c, fileids=".*\.txt") 
S_doc = [(pcr_S.words(fileid),c) for fileid in pcr_S.fileids()]

#C
c="Cok"
pcr_C = PlaintextCorpusReader(root=c, fileids=".*\.txt") 
C_doc = [(pcr_C.words(fileid),c) for fileid in pcr_C.fileids()]
#[(['CONSCIENTIOUSNESS', 'cautious', 'skeptical', ...], 'Cok')]

def document_features(document_words):
    document_words = set(document_words)
    features = {}
    for word in word_features:
        features['contains({})'.format(word)] = (word in document_words) 
    return features

c="Hermione"
pcr_her = PlaintextCorpusReader(root="./", fileids="Hermione.txt")
her_doc = [(pcr_her.words(fileid), c) for fileid in pcr_her.fileids()]
word_features = list(pcr_her.words())
her_features=[(document_features(d), c) for (d,c) in her_doc] 

c="Dudley"
pcr_dud = PlaintextCorpusReader(root="./", fileids="Dudley.txt")
dud_doc = [(pcr_dud.words(fileid),c) for fileid in pcr_dud.fileids()]
word_features = list(pcr_dud.words())
dud_features=[(document_features(d), c) for (d,c) in dud_doc] 

c="Harry"
pcr_ha = PlaintextCorpusReader(root="./", fileids="Harry.txt")
ha_doc = [(pcr_ha.words(fileid),c) for fileid in pcr_ha.fileids()]
word_features = list(pcr_ha.words())
ha_feature = [(document_features(d), c) for (d,c) in ha_doc] 


c="Malfoy"
pcr_mal = PlaintextCorpusReader(root="./", fileids="Malfoy.txt")
mal_doc = [(pcr_mal.words(fileid),c) for fileid in pcr_mal.fileids()]
word_features = list(pcr_mal.words())
mal_features = [(document_features(d), c) for (d,c) in mal_doc] 


documents = D_doc + I_doc + S_doc + C_doc
random.shuffle(x=documents) 

N_features =100
all_words = FreqDist(pcr_D.words() + pcr_I.words() + pcr_S.words() + pcr_C.words())   
#word_features = list(all_words)[:N_features] 
#print(word_features)

N_testing = 20
featuresets = [(document_features(d), c) for (d,c) in documents] 
#print(featuresets[:1])

#her_features=(document_features(d), c) for (d,c) in her_doc
#print(her_features)

  
train_set, test_set = featuresets[N_testing:], featuresets[:N_testing]


classifier = NaiveBayesClassifier.train(train_set) 

print("Hermione: %s" %classifier.classify(her_features)) 
#her_features 在 70 行 
#'list' object has no attribute 'keys'
